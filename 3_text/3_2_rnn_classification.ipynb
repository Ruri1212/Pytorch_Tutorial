{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['data/names/Spanish.txt', 'data/names/Arabic.txt', 'data/names/French.txt', 'data/names/Greek.txt', 'data/names/Dutch.txt', 'data/names/German.txt', 'data/names/Scottish.txt', 'data/names/English.txt', 'data/names/Czech.txt', 'data/names/Chinese.txt', 'data/names/Korean.txt', 'data/names/Polish.txt', 'data/names/Italian.txt', 'data/names/Russian.txt', 'data/names/Irish.txt', 'data/names/Vietnamese.txt', 'data/names/Portuguese.txt', 'data/names/Japanese.txt']\n",
      "all_letters   type:<class 'str'>, len: 57\n",
      "Slusarski\n",
      "Spanish\n",
      "['Abana', 'Abano', 'Abarca', 'Abaroa', 'Abascal', 'Abasolo', 'Abel', 'Abello', 'Aberquero', 'Abreu', 'Acosta', 'Agramunt', 'Aiza', 'Alamilla', 'Albert', 'Albuquerque', 'Aldana', 'Alfaro', 'Alvarado', 'Alvarez', 'Alves', 'Amador', 'Andreu', 'Antunez', 'Aqua', 'Aquino', 'Araujo', 'Araullo', 'Araya', 'Arce', 'Arechavaleta', 'Arena', 'Aritza', 'Armando', 'Arreola', 'Arriola', 'Asis', 'Asturias', 'Avana', 'Azarola', 'Banderas', 'Barros', 'Basurto', 'Bautista', 'Bello', 'Belmonte', 'Bengochea', 'Benitez', 'Bermudez', 'Blanco', 'Blanxart', 'Bolivar', 'Bonaventura', 'Bosque', 'Bustillo', 'Busto', 'Bustos', 'Cabello', 'Cabrera', 'Campo', 'Campos', 'Capello', 'Cardona', 'Caro', 'Casales', 'Castell', 'Castellano', 'Castillion', 'Castillo', 'Castro', 'Chavarria', 'Chavez', 'Colon', 'Costa', 'Crespo', 'Cruz', 'Cuellar', 'Cuevas', \"D'cruz\", \"D'cruze\", 'De la cruz', 'De la fuente', 'Del bosque', 'De leon', 'Delgado', 'Del olmo', 'De santigo', 'Diaz', 'Dominguez', 'Duarte', 'Durante', 'Echevarria', 'Echeverria', 'Elizondo', 'Escamilla', 'Escarcega', 'Escarra', 'Esparza', 'Espina', 'Espino', 'Espinosa', 'Espinoza', 'Estevez', 'Etxebarria', 'Etxeberria', 'Felix', 'Fernandez', 'Ferrer', 'Fierro', 'Flores', 'Fonseca', 'Franco', 'Fuentes', 'Gallego', 'Gallo', 'Garcia', 'Garrastazu', 'Garza', 'Gaspar', 'Gebara', 'Gomez', 'Gonzales', 'Gonzalez', 'Grec', 'Guadarrama', 'Guerra', 'Guerrero', 'Gutierrez', 'Gutierrez', 'Hernandez', 'Herrera', 'Herrero', 'Hierro', 'Holguin', 'Huerta', 'Ibanez', 'Ibarra', 'Iniguez', 'Iturburua', 'Jaso', 'Jasso', 'Jimenez', 'Jorda', 'Juarez', 'Lobo', 'Lopez', 'Losa', 'Loyola', 'Machado', 'Macias', 'Maradona', 'Maria', 'Marino', 'Marquez', 'Martell', 'Marti', 'Martinez', 'Martinez', 'Mas', 'Mata', 'Mateu', 'Medina', 'Melendez', 'Mendez', 'Mendoza', 'Menendez', 'Merlo', 'Michel', 'Mingo', 'Moles', 'Molina', 'Montero', 'Morales', 'Moralez', 'Moreno', 'Narvaez', 'Nieves', 'Noguerra', 'Nunez', 'Obando', 'Ochoa', 'Ojeda', 'Ola', 'Oleastro', 'Olguin', 'Oliver', 'Olmos', 'Oquendo', 'Orellana', 'Oriol', 'Ortega', 'Ortiz', 'Palomo', 'Paredes', 'Pavia', 'Pelaez', 'Pena', 'Perez', 'Perez', 'Petit', 'Picasso', 'Porra', 'Porras', 'Prieto', 'Puerta', 'Puga', 'Puig', 'Quinones', 'Quintana', 'Quiros', 'Ramirez', 'Ramos', 'Rana', 'Rendon', 'Rey', 'Reyes', 'Rios', 'Rivera', 'Rivero', 'Robledo', 'Robles', 'Rocha', 'Rodriguez', 'Rodriquez', 'Roig', 'Rojas', 'Rojo', 'Roldan', 'Roma', 'Roma', 'Romero', 'Rosa', 'Rosales', 'Rubio', 'Ruiz', 'Sala', 'Salamanca', 'Salazar', 'Salcedo', 'Salinas', 'Sanchez', 'Sandoval', 'San nicolas', 'Santana', 'Santiago', 'Santillian', 'Santos', 'Sastre', 'Sepulveda', 'Sierra', 'Silva', 'Soler', 'Solo', 'Solos', 'Soto', 'Suarez', 'Suero', 'Tapia', 'Terrazas', 'Tomas', 'Torres', 'Tos', 'Tosell', 'Toset', 'Travieso', 'Trujillo', 'Ubina', 'Urbina', 'Urena', 'Valdez', 'Valencia', 'Varela', 'Vargas', 'Vasquez', 'Vazquez', 'Vega', 'Vela', 'Vela', 'Velazquez', 'Ventura', 'Vicario', 'Vilaro', 'Villa', 'Villalobos', 'Villanueva', 'Villaverde', 'Viola', 'Viteri', 'Vivas', 'Vives', 'Ybarra', 'Zabala', 'Zambrano', 'Zamorano', 'Zapatero', 'Zavala', 'Zubizarreta', 'Zuniga']\n"
     ]
    }
   ],
   "source": [
    "from __future__ import unicode_literals, print_function, division\n",
    "from io import open\n",
    "import glob\n",
    "import os\n",
    "\n",
    "def findFiles(path): return glob.glob(path)\n",
    "\n",
    "print(findFiles('data/names/*.txt'))\n",
    "\n",
    "import unicodedata\n",
    "import string\n",
    "\n",
    "all_letters = string.ascii_letters + \" .,;'\"\n",
    "n_letters = len(all_letters)\n",
    "### all_letters = a~z+A~Z+ .,;' (記号)\n",
    "print(f\"all_letters   type:{type(all_letters)}, len: {len(all_letters)}\")\n",
    "\n",
    "\n",
    "# Turn a Unicode string to plain ASCII, thanks to https://stackoverflow.com/a/518232/2809427\n",
    "def unicodeToAscii(s):\n",
    "    return ''.join(\n",
    "        c for c in unicodedata.normalize('NFD', s)\n",
    "        if unicodedata.category(c) != 'Mn'\n",
    "        and c in all_letters\n",
    "    )\n",
    "\n",
    "print(unicodeToAscii('Ślusàrski'))\n",
    "\n",
    "# Build the category_lines dictionary, a list of names per language\n",
    "category_lines = {}\n",
    "all_categories = []\n",
    "\n",
    "# Read a file and split into lines\n",
    "def readLines(filename):\n",
    "    lines = open(filename, encoding='utf-8').read().strip().split('\\n')\n",
    "    return [unicodeToAscii(line) for line in lines]\n",
    "\n",
    "for index,filename in enumerate(findFiles('data/names/*.txt')):\n",
    "    ### basename  - *.txt (メインのファイル名)\n",
    "    ### splittext - 拡張子を分割 [0]をとることで拡張子削除\n",
    "    category = os.path.splitext(os.path.basename(filename))[0]\n",
    "    all_categories.append(category)\n",
    "    lines = readLines(filename)\n",
    "    if index == 0:\n",
    "        print(category)\n",
    "        print(lines)\n",
    "    category_lines[category] = lines\n",
    "\n",
    "n_categories = len(all_categories)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "18\n",
      "['Spanish', 'Arabic', 'French']\n",
      "['Abandonato', 'Abatangelo', 'Abatantuono', 'Abate', 'Abategiovanni']\n"
     ]
    }
   ],
   "source": [
    "print(n_categories)\n",
    "\n",
    "print(all_categories[:3])\n",
    "\n",
    "print(category_lines['Italian'][:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0.]])\n",
      "torch.Size([5, 1, 57])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "# Find letter index from all_letters, e.g. \"a\" = 0\n",
    "def letterToIndex(letter):\n",
    "    return all_letters.find(letter)\n",
    "\n",
    "# Just for demonstration, turn a letter into a <1 x n_letters> Tensor\n",
    "def letterToTensor(letter):\n",
    "    tensor = torch.zeros(1, n_letters)\n",
    "    tensor[0][letterToIndex(letter)] = 1\n",
    "    return tensor\n",
    "\n",
    "# Turn a line into a <line_length x 1 x n_letters>,\n",
    "# or an array of one-hot letter vectors\n",
    "def lineToTensor(line):\n",
    "    tensor = torch.zeros(len(line), 1, n_letters)\n",
    "    for li, letter in enumerate(line):\n",
    "        tensor[li][0][letterToIndex(letter)] = 1\n",
    "    return tensor\n",
    "\n",
    "print(letterToTensor('J'))\n",
    "\n",
    "print(lineToTensor('Jones').size())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.9079, 0.8682, 0.1551],\n",
      "        [0.8136, 0.1870, 0.1724]]) torch.Size([2, 3])\n",
      "tensor([[0, 1, 2, 3],\n",
      "        [4, 5, 6, 7]]) torch.Size([2, 4])\n",
      "tensor([[0.9079, 0.8682, 0.1551, 0.0000, 1.0000, 2.0000, 3.0000],\n",
      "        [0.8136, 0.1870, 0.1724, 4.0000, 5.0000, 6.0000, 7.0000]])\n"
     ]
    }
   ],
   "source": [
    "a = torch.rand((2,3))\n",
    "print(a,a.shape)\n",
    "b = torch.arange(8).reshape(2,4)\n",
    "print(b,b.shape)\n",
    "\n",
    "c = torch.cat((a,b),1)\n",
    "print(c)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "\n",
    "class RNN(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, output_size):\n",
    "        super(RNN, self).__init__()\n",
    "\n",
    "        self.hidden_size = hidden_size\n",
    "\n",
    "        self.i2h = nn.Linear(input_size + hidden_size, hidden_size)\n",
    "        self.i2o = nn.Linear(input_size + hidden_size, output_size)\n",
    "        self.softmax = nn.LogSoftmax(dim=1)\n",
    "\n",
    "    def forward(self, input, hidden):\n",
    "        combined = torch.cat((input, hidden), 1)\n",
    "        hidden = self.i2h(combined)\n",
    "        output = self.i2o(combined)\n",
    "        output = self.softmax(output)\n",
    "        return output, hidden\n",
    "\n",
    "    def initHidden(self):\n",
    "        return torch.zeros(1, self.hidden_size)\n",
    "\n",
    "n_hidden = 128\n",
    "rnn = RNN(n_letters, n_hidden, n_categories)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[-2.9844, -2.9482, -2.8458, -2.8929, -2.8659, -2.9100, -2.9781, -2.8027,\n",
      "         -2.9244, -2.9009, -2.9635, -2.9224, -2.8271, -2.8009, -2.8325, -2.9573,\n",
      "         -2.9488, -2.7603]], grad_fn=<LogSoftmaxBackward0>) torch.Size([1, 18])\n",
      "torch.return_types.topk(\n",
      "values=tensor([[-2.7603, -2.8009, -2.8027]], grad_fn=<TopkBackward0>),\n",
      "indices=tensor([[17, 13,  7]]))\n",
      "-2.7602930068969727\n"
     ]
    }
   ],
   "source": [
    "input = lineToTensor('Albert')\n",
    "hidden = torch.zeros(1, n_hidden)\n",
    "\n",
    "output, next_hidden = rnn(input[0], hidden)\n",
    "print(output,output.shape)\n",
    "\n",
    "print(output.topk(3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[-2.7603]], grad_fn=<TopkBackward0>) tensor([[17]])\n",
      "('Japanese', 17)\n"
     ]
    }
   ],
   "source": [
    "def categoryFromOutput(output):\n",
    "    top_n, top_i = output.topk(1)\n",
    "    print(top_n,top_i)\n",
    "    category_i = top_i[0].item()\n",
    "    return all_categories[category_i], category_i\n",
    "\n",
    "print(categoryFromOutput(output))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "print(random.randint(0,4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Italian <class 'str'>\n",
      "category = Italian / line = Garfagnini\n",
      "Italian <class 'str'>\n",
      "category = Italian / line = Cremonesi\n",
      "French <class 'str'>\n",
      "category = French / line = Vincent\n",
      "Vietnamese <class 'str'>\n",
      "category = Vietnamese / line = Van\n",
      "German <class 'str'>\n",
      "category = German / line = Martin\n",
      "English <class 'str'>\n",
      "category = English / line = Myers\n",
      "Russian <class 'str'>\n",
      "category = Russian / line = Bakhmutski\n",
      "Vietnamese <class 'str'>\n",
      "category = Vietnamese / line = Tong\n",
      "Scottish <class 'str'>\n",
      "category = Scottish / line = Robertson\n",
      "Korean <class 'str'>\n",
      "category = Korean / line = Hung\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "\n",
    "### lはリストを受け取る．つまり返り値はその要素\n",
    "def randomChoice(l):\n",
    "    return l[random.randint(0, len(l) - 1)]\n",
    "\n",
    "def randomTrainingExample():\n",
    "    category = randomChoice(all_categories)\n",
    "    print(category,type(category))\n",
    "    line = randomChoice(category_lines[category])\n",
    "    category_tensor = torch.tensor([all_categories.index(category)], dtype=torch.long)\n",
    "    line_tensor = lineToTensor(line)\n",
    "    return category, line, category_tensor, line_tensor\n",
    "\n",
    "for i in range(10):\n",
    "    category, line, category_tensor, line_tensor = randomTrainingExample()\n",
    "    print('category =', category, '/ line =', line)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "deeplearning",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
